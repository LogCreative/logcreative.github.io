<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="/favicon.ico">
    <title>åœ¨ Blackwell æ¶æ„ä¸Šç¼–è¯‘å¤§æ¨¡å‹ç›¸å…³æ¡†æ¶</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://unpkg.com/prismjs@1.20.0/themes/prism.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/css/lcard.css"></head>
<body>
    <div id="navigation">
        <div class="widthlimit">
        <a href="/"><img src="/assets/images/LC.gif" height="64px"></a>
        <ul><li>
                    <a href="/">ä¸»é¡µ</a>
                </li><li>
                    <a href="/blogs">åšå®¢</a>
                </li><li>
                    <a href="/archive">å­˜å‚¨åº“</a>
                </li><li>
                    <a href="/notes">è¯¾ç¨‹è®°å½•</a>
                </li><li>
                    <a href="/about">å…³äº</a>
                </li></ul>
        </div>
    </div>

    
<div id="main" class="widthlimit">

    <h1>åœ¨ Blackwell æ¶æ„ä¸Šç¼–è¯‘å¤§æ¨¡å‹ç›¸å…³æ¡†æ¶</h1>

    <h2>RTX 5070 Ti æ˜¾å¡åˆšåˆšä¸Šå¸‚ï¼Œä½†æ˜¯ç›¸å…³æ¡†æ¶å°šæœªé€‚é…ï¼Œå¦‚ä½•å®‰è£…æœ€æ–°æ˜¾å¡é©±åŠ¨ï¼Œcudaï¼Œpytorch, triton, flash-attention ç­‰ã€‚</h2>

    <h2>2025-03-23</h2>

    <a href="https://cloud.tencent.com/act/cps/redirect?redirect=6150&cps_key=3b86b83ec1516f560c5dc351fe35065b&from=console"><img src="/assets/images/rhino-design-1200x90x2.png" alt="ã€è…¾è®¯äº‘ã€‘2æ ¸2Gäº‘æœåŠ¡å™¨æ–°è€åŒäº« 99å…ƒ/å¹´ï¼Œç»­è´¹åŒä»·ï¼Œäº‘æœåŠ¡å™¨3å¹´æœº/5å¹´æœºé™æ—¶æŠ¢è´­ï¼Œä½è‡³ 2.5æŠ˜" style="max-width:100%;cursor:pointer;"></img></a>

    <h3>æ–°ç³»åˆ—æ˜¾å¡</h3>
<p>ç”±äºæˆ‘è‡ªå¤§å­¦ä»¥æ¥éƒ½æ˜¯ä½¿ç”¨çš„è½»è–„æœ¬ï¼Œä¸€å°æ˜¯ Surface Pro 5ï¼Œä¸€å°æ˜¯ MacBook M1 Proï¼Œå®ƒä»¬éƒ½æ²¡æœ‰ç‹¬ç«‹æ˜¾å¡ï¼Œè€Œå®¶ä¸­çš„ GTX 1050 Ti ä¹Ÿæ˜¯å¾ˆä¹…ä»¥å‰åªæœ‰ 4GB æ˜¾å­˜çš„æ™®é€šæ˜¾å¡ï¼Œåœ¨è¿™ä¸ªäººå·¥æ™ºèƒ½çš„æ—¶ä»£ï¼Œè¿™ç§ç¡¬ä»¶é…ç½®æ˜¯ä¸æ€ä¹ˆå¤Ÿçš„ï¼šå¤§æ¨¡å‹çš„ç§‘ç ”å­¦ä¹ éœ€è¦ç‹¬ç«‹æ˜¾å¡è¿›è¡Œè®­ç»ƒæ¨ç†ï¼Œç©æ¸¸æˆä¹Ÿéœ€è¦é«˜æ€§èƒ½æ˜¾å¡åŸºäºæ›´å…ˆè¿›çš„å…‰è¿½æŠ€æœ¯æä¾›æƒŠè‰³çš„ç”»é¢ã€‚å› æ­¤å¾ˆä¹…ä»¥æ¥ï¼Œæˆ‘éƒ½å¸Œæœ›æœ‰ä¸€å¼ ç‹¬äº«çš„è‹±ä¼Ÿè¾¾é«˜æ€§èƒ½æ˜¾å¡ã€‚åœ¨ 40 ç³»æ˜¾å¡ä¸Šå¸‚ä¸¤å¹´ä¹‹åï¼Œ50 ç³»æ˜¾å¡ç»ˆäºè¢«ç«¯ä¸Šæ¥äº†ï¼Œè™½ç„¶å„è·¯è¯„æµ‹éƒ½è®¤ä¸ºè¿™ä»£åœ¨æ¸¸æˆæ–¹é¢çš„æå‡å·®å¼ºäººæ„ï¼Œä½†æ˜¯ç”±äºè€é»„åœ¨<s>æ‹¼å¥½å¸§</s> DLSS 4 è¿™ç§åŸºäº Transformers çš„ç»“æ„å‘å±•ä½¿å¾—è¿™ä»£æ˜¾å¡åœ¨ AI ä¸Šçš„æ€§èƒ½æ˜æ˜¾æœ‰äº†æ›´å¤šçš„åå‘æ€§ã€‚äºæ˜¯ï¼Œé¡¶ç€æº¢ä»·ï¼Œå…¥æ‰‹äº†äº¬ä¸œè‡ªè¥çš„å¸¦æœ‰ 16 GB æ˜¾å­˜ã€CUDA æ•° 8960 çš„é­”é¹° 5070 Tiï¼ˆå±äºæ˜¯ buff æ‹‰æ»¡äº†ï¼Œä½†æ˜¯è€ƒè™‘åˆ°æœ€è¿‘ ROPS å…‰æ …å•å…ƒé£æ³¢ï¼Œè¿˜æ˜¯ä¹°æ­£ç‰Œè´§ç›¸å¯¹ä¿é™©ä¸€äº›ï¼‰ã€‚</p>
<p>è™½ç„¶ Blackwell è‡ªä¸“ä¸šè®¡ç®—æ˜¾å¡ H100 å°±æœ‰äº†ï¼Œä½†æ˜¯ä¸€ç›´ä»¥æ¥éƒ½æ²¡æœ‰åœ¨æ¶ˆè´¹çº§ä¸Šé“ºå¼€ï¼Œæ‰€ä»¥å¯¹äºæ–°æ˜¾å¡ï¼Œå„ç§å¤§æ¨¡å‹ç›¸å…³æ¡†æ¶çš„é€‚é…éƒ½å°šä¸æˆç†Ÿï¼Œä¹Ÿå°±ä¸èƒ½ç›´æ¥å³è£…å³ç”¨ï¼Œè¿™å¯¹ AI æ–¹é¢ä¸æ˜¯å¾ˆå‹å¥½çš„ã€‚å¹¸è¿çš„æ˜¯ï¼Œéƒ¨åˆ†æ¡†æ¶å·²ç»æœ‰äº†å¯ä»¥æµ‹è¯•çš„ç‰ˆæœ¬ï¼Œé€šè¿‡æ‰‹åŠ¨çš„æºä»£ç ç¼–è¯‘è‡³å°‘å¯ä»¥è·‘èµ·æ¥ä»£ç ï¼Œç”±äºæœ€è¿‘è¿˜æ²¡æœ‰ç›¸å…³çš„èµ„æ–™ï¼Œæ‰€ä»¥åˆ†äº«å‡ºæ¥ä»¥ä¾›å‚è€ƒï¼Œè¿‡ä¸€æ®µæ—¶é—´ä¼°è®¡ä¹Ÿå°±ä¸ç”¨è¿™ä¹ˆéº»çƒ¦äº†ã€‚</p>
<h3>ç¯å¢ƒé…ç½®</h3>
<p>å¯¹äº AI ç ”ç©¶è€Œè¨€ï¼Œä½¿ç”¨ Linux æ“ä½œç³»ç»Ÿæ— ç–‘æ˜¯æœ€å¥½çš„é€‰æ‹©ï¼Œä½†æ˜¯ä¸ºäº†ä¸æ‰“æ‰°æˆ‘æ‰“æ¸¸æˆï¼ŒæŒ‰ç…§<a href="https://www.digitaltrends.com/computing/how-to-dual-boot-linux-and-windows/#dt-heading-how-to-install-linux">æ•™ç¨‹</a>å®‰è£…äº† Windows 11-Ubuntu 24.10 åŒç³»ç»Ÿï¼Œåœ¨ Ubuntu ç³»ç»Ÿä¸Šé€šè¿‡ <a href="https://github.com/fatedier/frp?tab=readme-ov-file#access-your-computer-in-a-lan-network-via-ssh">frp</a> å·¥å…·å®ç°äº†è¿œç¨‹ SSH å†…ç½‘è¿æ¥ï¼ˆè¿™ä¸€å—æ˜¯é€šè¿‡ <code>systemctl</code> ç®¡ç†æœåŠ¡æ¥å®ç°è‡ªå¯åŠ¨çš„ï¼Œæ„Ÿå…´è¶£å¯ä»¥å‚è§ <a href="https://zhuanlan.zhihu.com/p/521448626">è¿™ç¯‡åšæ–‡</a>ï¼‰ã€‚</p>
<h3>æ˜¾å¡é©±åŠ¨</h3>
<p>å¼€å§‹è¿›å…¥æ­£é¢˜ï¼Œé¦–å…ˆç¬¬ä¸€ä¸ªéšœç¢å°±æ˜¯æ˜¾å¡é©±åŠ¨ã€‚ç”±äºè¯¥å‹å·çš„æ˜¾å¡å¤ªæ–°äº†ï¼Œæš‚æ—¶è¿˜æ— æ³•é€šè¿‡ Ubuntu è‡ªå¸¦çš„è½¯ä»¶ä¸æ›´æ–°è·å–åˆ°æ˜¾å¡é©±åŠ¨ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨åœ°æ·»åŠ è½¯ä»¶æ›´æ–°æºå®‰è£…é©±åŠ¨ã€‚</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> build-essential<br><span class="token function">sudo</span> add-apt-repository ppa:graphics-drivers/ppa<br><span class="token function">sudo</span> <span class="token function">apt</span> update</code></pre>
<p>ç„¶åï¼Œéœ€è¦å…³é—­ä¸å›¾å½¢æ˜¾ç¤ºç›¸å…³çš„æ¡Œé¢ç¯å¢ƒï¼Œè¿›å…¥å†…ç½®ç»ˆç«¯ç•Œé¢å®‰è£…ï¼ˆè¿™æ­¥å¾ˆé‡è¦ï¼Œå¦åˆ™æ˜¾å¡é©±åŠ¨å®‰è£…è¿‡ç¨‹ä¼šå¡æ­»ï¼Œç„¶åå°±å¾—é‡è£…ç³»ç»Ÿäº†ï¼‰ã€‚ä¹Ÿå°±æ˜¯ä½¿ç”¨ <kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>F3</kbd> è¿›å…¥ tty3ã€‚ç™»å½•ä¹‹åï¼Œå…³é—­æ¡Œé¢ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl stop gdm3  <span class="token comment"># gnome</span></code></pre>
<p>ç„¶åå¼€å§‹å®‰è£…é©±åŠ¨ï¼Œåœ¨å®‰è£…ç•Œé¢ä¸­æš‚æ—¶ä½¿ç”¨ MIT é©±åŠ¨ï¼Œä¸“æœ‰é©±åŠ¨ç»è¿‡éªŒè¯æš‚æ—¶æ— æ³•è¢« <code>nvidia-smi</code> æŒ‡ä»¤æ£€æµ‹åˆ°ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> nvidia-driver-570</code></pre>
<p>ç­‰å¾…å®‰è£…å®Œæ¯•åï¼Œé‡å¯ç³»ç»Ÿï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">reboot</span></code></pre>
<p>ä¹‹åé€šè¿‡ <code>nvidia-smi</code> å‘½ä»¤éªŒè¯ï¼Œå¦‚æœæ£€æµ‹åˆ°æ˜¾å¡å°±è¯´æ˜å®‰è£…æˆåŠŸäº†ã€‚</p>
<pre class="language-text"><code class="language-text">Sun Mar 23 22:15:50 2025<br>+-----------------------------------------------------------------------------------------+<br>| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |<br>|-----------------------------------------+------------------------+----------------------+<br>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |<br>|                                         |                        |               MIG M. |<br>|=========================================+========================+======================|<br>|   0  NVIDIA GeForce RTX 5070 Ti     Off |   00000000:01:00.0  On |                  N/A |<br>|  0%   27C    P5             38W /  300W |     130MiB /  16303MiB |      0%      Default |<br>|                                         |                        |                  N/A |<br>+-----------------------------------------+------------------------+----------------------+<br><br>+-----------------------------------------------------------------------------------------+<br>| Processes:                                                                              |<br>|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |<br>|        ID   ID                                                               Usage      |<br>|=========================================================================================|<br>|  No running processes found                                                             |<br>+-----------------------------------------------------------------------------------------+</code></pre>
<h3>CUDA</h3>
<p>æ ¹æ®è¿™ç¯‡é‡è¦çš„<a href="https://forums.developer.nvidia.com/t/software-migration-guide-for-nvidia-blackwell-rtx-gpus-a-guide-to-cuda-12-8-pytorch-tensorrt-and-llama-cpp/321330">å®˜æ–¹è¯´æ˜</a>ï¼Œç°åœ¨ 50 ç³»æ˜¾å¡åªèƒ½ä½¿ç”¨ CUDA 12.8ï¼Œæ‰€ä»¥æŒ‰ç…§<a href="https://developer.nvidia.com/cuda-downloads/?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=24.04&amp;target_type=deb_local">å®˜æ–¹å®‰è£…è¯´æ˜</a>ä¸‹è½½å®‰è£…ï¼Œå®‰è£…çš„é‡è¦å‘½ä»¤å¦‚ä¸‹å¯ä¾›å‚è€ƒï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin<br><span class="token function">sudo</span> <span class="token function">mv</span> cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600<br><span class="token function">wget</span> https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda-repo-ubuntu2404-12-8-local_12.8.1-570.124.06-1_amd64.deb<br><span class="token function">sudo</span> dpkg <span class="token parameter variable">-i</span> cuda-repo-ubuntu2404-12-8-local_12.8.1-570.124.06-1_amd64.deb<br><span class="token function">sudo</span> <span class="token function">cp</span> /var/cuda-repo-ubuntu2404-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/<br><span class="token function">sudo</span> <span class="token function">apt-get</span> update<br><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token parameter variable">-y</span> <span class="token function">install</span> cuda-toolkit-12-8</code></pre>
<p>ä¿é™©èµ·è§ï¼Œå®‰è£…å®Œæˆåï¼Œå°† CUDA åŠ å…¥è·¯å¾„ä¸­ï¼ˆå¯ä»¥ä¸´æ—¶ä¿®æ”¹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ <code>~/.bashrc</code> æ°¸ä¹…ä¿®æ”¹ï¼‰ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token string">"/usr/local/cuda-12.8/bin:<span class="token environment constant">$PATH</span>"</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span><span class="token string">"/usr/local/cuda-12.8"</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">LIBRARY_PATH</span><span class="token operator">=</span><span class="token string">"/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64/stubs:<span class="token variable">$LIBRARY_PATH</span>"</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token string">"/usr/local/cuda-12.8/lib64:<span class="token variable">$LD_LIBRARY_PATH</span>"</span></code></pre>
<p>æ–°å»ºç»ˆç«¯ï¼Œæˆ–è€… <code>source ~/.bashrc</code> ç”Ÿæ•ˆã€‚</p>
<h3>Anaconda</h3>
<p>ä¸ºäº†éš”ç¦»ä¸åŒçš„ Python ç¯å¢ƒï¼ˆç³»ç»Ÿè‡ªå¸¦çš„ Python ç¯å¢ƒç”±äºä¿æŠ¤åŸå› ï¼Œä¸€èˆ¬ä¸èƒ½ç›´æ¥é€šè¿‡ <code>pip</code> å®‰è£…ï¼‰ï¼Œå»ºè®®å®‰è£… <a href="https://www.anaconda.com/download/success">anaconda</a> è¿›è¡Œç¯å¢ƒç®¡ç†ï¼ˆå®é™…ä¸Š conda ä¹Ÿå¯ä»¥å®‰è£… CUDAï¼Œå¦‚æœä½ éœ€è¦å®‰è£…å¤šä¸ªç‰ˆæœ¬çš„ CUDA çš„è¯ï¼Œå‚è§ <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#conda-installation">CUDA å®‰è£…æ–‡æ¡£</a>ï¼‰ï¼Œä»¥ä¸‹æˆªå–è‡³ <a href="https://www.anaconda.com/docs/getting-started/anaconda/install#macos-linux-installation">Anaconda å®‰è£…è¯´æ˜</a>ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> <span class="token parameter variable">-O</span> https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh<br><span class="token function">bash</span> ~/Anaconda3-2024.10-1-Linux-x86_64.sh<br><span class="token comment"># ä»¥ä¸‹è¯´æ˜ä½¿ç”¨äº†é»˜è®¤å®‰è£…ä½ç½®</span><br><span class="token builtin class-name">source</span> ~/.bashrc</code></pre>
<p>å®‰è£…ç»“æŸåï¼Œé€šè¿‡ä¸‹é¢çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªæ–°ç¯å¢ƒï¼š</p>
<pre class="language-bash"><code class="language-bash">conda create <span class="token parameter variable">-n</span> llm <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.12</span></code></pre>
<p>è¿™é‡Œä½¿ç”¨ Python 3.12 ç‰ˆæœ¬æ˜¯å› ä¸ºä¸€äº›å¾ˆæ–°çš„åŒ…è¦æ±‚çš„ Python ç‰ˆæœ¬æ˜¯å¾ˆé«˜çš„ï¼ˆæ¯”å¦‚ <a href="https://github.com/langchain-ai/langchain"><code>langchain</code></a>ï¼‰ï¼Œä½¿ç”¨æ›´æ–°ç‰ˆæœ¬çš„ Python ä¹Ÿèƒ½å¤Ÿä½¿ç”¨ä¸€äº›ç›¸å¯¹æ›´æ–°æ›´é¡ºæ‰‹çš„è¯­æ³•ã€‚</p>
<p>åˆ›å»ºå®Œæ¯•ä¹‹åï¼Œå°±å¯ä»¥è¿›å…¥è¯¥ç¯å¢ƒï¼š</p>
<pre class="language-bash"><code class="language-bash">conda activate llm</code></pre>
<p>ä¹‹åå°±å¯ä»¥åœ¨è¯¥ç¯å¢ƒä¸‹è¿›è¡Œå®‰è£…ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token punctuation">(</span>llm<span class="token punctuation">)</span> $ </code></pre>
<h3>Pytorch</h3>
<h4>torch</h4>
<p>æŒ‰ç…§è¿™ç¯‡é‡è¦çš„<a href="https://forums.developer.nvidia.com/t/software-migration-guide-for-nvidia-blackwell-rtx-gpus-a-guide-to-cuda-12-8-pytorch-tensorrt-and-llama-cpp/321330">å®˜æ–¹è¯´æ˜</a>çš„è¯´æ³•ï¼Œå®é™…ä¸Šå·²ç»å¯ä»¥ç›´æ¥ä½¿ç”¨æœ€æ–°çš„é¢„ç¼–è¯‘äºŒè¿›åˆ¶çš„ Pytorch è¿›è¡Œå®‰è£…äº†ï¼š<code>pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128</code>ï¼Œå¦‚æœä½ åªéœ€è¦è¿è¡Œéé‡åŒ–çš„å¤§æ¨¡å‹ï¼Œé‚£ä¹ˆå®‰è£…è¿™ä¸ªä¹Ÿå°±è¶³å¤Ÿã€‚</p>
<p>å¦åˆ™ï¼Œè€ƒè™‘åˆ° <code>triton</code> ä»¥åŠåç»­çš„ <code>vllm</code> éƒ½ä¼šä¾èµ–äº Pytorch 2.6.0 ç‰ˆæœ¬ï¼Œæ ¹æ® <a href="https://github.com/triton-lang/triton?tab=readme-ov-file#enabling-blackwell-support"><code>triton</code> çš„å®˜æ–¹è¯´æ˜</a>ï¼Œä¸ºäº†å¯¹è€ç‰ˆæœ¬ä½¿ç”¨ CUDA 12.8ï¼Œéœ€è¦å¯¹ Pytorch è¿™ä¸ªç‰ˆæœ¬è¿›è¡Œæºç ç¼–è¯‘å®‰è£…ã€‚</p>
<p>åœ¨å¼€å§‹æºç ç¼–è¯‘ä¹‹å‰ï¼Œé¦–å…ˆå®‰è£…ä¸€äº›ç¼–è¯‘å¿…éœ€å“ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cmake <span class="token function">git</span> ninja</code></pre>
<p>ç„¶åæ‹‰å–ä»£ç è¿›è¡Œé¢„å¤‡å·¥ä½œï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token comment"># æ‹‰å– pytorch 2.6.0-rc9 ç‰ˆæœ¬ï¼Œåªæ‹‰å–è¯¥ç‰ˆæœ¬è€Œä¸æ‹‰å–å…¨éƒ¨</span><br><span class="token function">git</span> clone https://github.com/pytorch/pytorch <span class="token parameter variable">-b</span> v2.6.0-rc9 <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> pytorch<br><span class="token function">git</span> submodule <span class="token function">sync</span><br><span class="token function">git</span> submodule update <span class="token parameter variable">--init</span> <span class="token parameter variable">--recursive</span> <span class="token parameter variable">-j</span> <span class="token number">8</span><br><br><span class="token comment"># å®‰è£… pytorch çš„å…¶ä»–ä¾èµ–åŒ…</span><br>pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt<br>pip <span class="token function">install</span> mkl-static mkl-include wheel</code></pre>
<p>è™½ç„¶ä¹‹å‰å·²ç»å®‰è£…äº† <code>build-essential</code> åŒ…å«äº† <code>gcc</code> ç¼–è¯‘å™¨ï¼Œä½†æ˜¯ Ubuntu 24.04 é»˜è®¤å®‰è£…çš„ç‰ˆæœ¬æ˜¯ <code>gcc-14</code>ï¼Œæ ¹æ®è¿™ä¸ªå°šæœªè¢«è§£å†³çš„ Issue <a href="https://github.com/pytorch/pytorch/issues/129358">pytorch/pytorch#129358</a>ï¼ŒPytorch ç›®å‰å› ä¸ºä¾èµ–çš„ <code>fbgemm</code> ç¬¬ä¸‰æ–¹åº“ç‰ˆæœ¬è¿‡è€ä¼šå‡ºç°ç¼–è¯‘æŠ¥é”™ï¼ˆå³ä½¿æ›´æ–°äº†è¯¥åº“çš„ç‰ˆæœ¬ï¼Œç»è¿‡å°è¯•ä¹Ÿæ— æ³•åœ¨å³å°†å®‰è£…çš„è€ Pytorch ç¼–è¯‘æˆåŠŸï¼‰ï¼Œæ‰€ä»¥ä½ éœ€è¦å®‰è£…ä¸€ä¸ªé™çº§çš„ gcc ç‰ˆæœ¬ï¼Œå¹¶è®© CMAKE ä½¿ç”¨è¯¥ç‰ˆæœ¬çš„ gccï¼ˆPytorch æ–‡æ¡£ä¸­æåˆ°çš„ <code>CC</code> ç¯å¢ƒå˜é‡è®¾ç½®æ˜¯ä¸å¤Ÿçš„ï¼Œè¿˜éœ€è¦è®¾ç½® <code>CXX</code>ï¼‰ï¼Œæˆªå–è‡ª<a href="https://github.com/pytorch/pytorch/issues/129358#issuecomment-2735364303">è¿™é‡Œ</a>ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> gcc-13 g++-13<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span>/usr/bin/gcc-13<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span>/usr/bin/g++-13</code></pre>
<p>ä¹‹åå¯èƒ½è¿˜éœ€è¦å¯¹ç¡¬ç¼–ç çš„ gcc ç¨‹åºè¿›è¡Œæ›´æ–°ï¼Œå°†<a href="https://github.com/pytorch/pytorch/blob/8bece886552e58b75a066226c1c7da7975d68ba6/setup.py#L791-L793">è¿™é‡Œ</a>æ”¹ä¸ºï¼š</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">for</span> command <span class="token keyword">in</span> all_commands<span class="token punctuation">:</span><br>    <span class="token keyword">if</span> command<span class="token punctuation">[</span><span class="token string">"command"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"gcc-13 "</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>        command<span class="token punctuation">[</span><span class="token string">"command"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"g++-13 "</span> <span class="token operator">+</span> command<span class="token punctuation">[</span><span class="token string">"command"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">:</span><span class="token punctuation">]</span></code></pre>
<p>æ·»åŠ ç¯å¢ƒå˜é‡ï¼Œå¼€å§‹ç¼–è¯‘ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token comment"># å¼€å§‹ç¼–è¯‘å®‰è£…</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-12.8<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_PATH</span><span class="token operator">=</span><span class="token variable">$CUDA_HOME</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">TORCH_CUDA_ARCH_LIST</span><span class="token operator">=</span>Blackwell<br>python setup.py develop<br><br><span class="token comment"># å¯é€‰åœ°å¯¼å‡º wheel äºŒè¿›åˆ¶ä¾›å¤‡ä»½ä½¿ç”¨</span><br>python setup.py bdist_wheel<br><span class="token function">ls</span> dist <span class="token comment"># äºŒè¿›åˆ¶æ–‡ä»¶åœ¨è¿™</span></code></pre>
<p>ç¼–è¯‘å®Œæˆåï¼Œåº”è¯¥å°±å¯ä»¥æ£€æµ‹åˆ°äº†ï¼š</p>
<pre class="language-bash"><code class="language-bash">$ pip list <span class="token operator">|</span> <span class="token function">grep</span> torch    <span class="token comment"># ä¸æ¸…æ¥šä¸ºä»€ä¹ˆæ˜¾ç¤º 2.7.0 ç‰ˆæœ¬</span><br>torch                             <span class="token number">2.7</span>.0.dev20250308+cu128</code></pre>
<h4>torchvision</h4>
<p>å®‰è£… <a href="https://github.com/pytorch/vision">torchvision</a> ä¹Ÿéœ€è¦ä»æºç å®‰è£…ï¼ŒæŒ‰ç…§ <a href="https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md">å®‰è£…è¯´æ˜</a> ä¸è¦å®‰è£…å¯é€‰é¡¹ï¼Œç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/pytorch/vision.git <span class="token parameter variable">-b</span> v0.21.0-rc8 <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> vision<br><span class="token comment"># è€ƒè™‘åˆ°ä¹‹å‰åˆ¶å®šçš„ç‰ˆæœ¬æ˜¯ GCC 13ï¼Œè¿™é‡Œä¹Ÿè¿›è¡ŒæŒ‡å®š</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span>/usr/bin/gcc-13<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span>/usr/bin/g++-13<br>python setup.py develop </code></pre>
<p>éªŒè¯å®‰è£…å®Œæˆï¼š</p>
<pre class="language-bash"><code class="language-bash">$ pip list <span class="token operator">|</span> <span class="token function">grep</span> torchvision<br>torchvision                       <span class="token number">0.21</span>.0+7af6987</code></pre>
<h4>torchaudio</h4>
<p>å®‰è£… <a href="https://github.com/pytorch/audio">torchaudio</a> åŒç†ï¼Œä½†æ˜¯å®‰è£…è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦åœ¨çº¿å®‰è£…ä¾èµ–ï¼Œæ³¨æ„è”ç½‘é—®é¢˜ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/pytorch/audio.git <span class="token parameter variable">-b</span> v2.6.0-rc7 <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> audio<br><span class="token comment"># è€ƒè™‘åˆ°ä¹‹å‰åˆ¶å®šçš„ç‰ˆæœ¬æ˜¯ GCC 13ï¼Œè¿™é‡Œä¹Ÿè¿›è¡ŒæŒ‡å®š</span><br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span>/usr/bin/gcc-13<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span>/usr/bin/g++-13<br>python setup.py develop</code></pre>
<p>éªŒè¯å®‰è£…å®Œæˆï¼š</p>
<pre class="language-bash"><code class="language-bash">$ pip list <span class="token operator">|</span> <span class="token function">grep</span> torchaudio<br>torchaudio                        <span class="token number">2.6</span>.0a0+d883142</code></pre>
<h3>triton</h3>
<p>ç”±äº <a href="https://pypi.org/project/autoawq/"><code>autoawq</code></a>ã€<a href="https://pypi.org/project/flash-attn/"><code>flash-attention</code></a>ã€<a href="https://pypi.org/project/deepspeed/"><code>deepspeed</code></a> éƒ½å¯èƒ½éœ€è¦ä¾èµ–äº <code>triton</code> åŒ…ï¼Œæ‰€ä»¥è¿™é‡Œé¦–å…ˆå®‰è£… <code>triton</code>ã€‚</p>
<p>ç»§ç»­ <a href="https://github.com/triton-lang/triton?tab=readme-ov-file#enabling-blackwell-support"><code>triton</code> çš„å®˜æ–¹è¯´æ˜</a>ï¼Œé¦–å…ˆå…‹éš†å­˜å‚¨åº“ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/triton-lang/triton.git <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> triton</code></pre>
<p>ç”±äºç›®å‰å¯èƒ½ä¼šå‡ºç° CUDA åº“é“¾æ¥é”™è¯¯çš„é—®é¢˜ï¼Œéœ€è¦è®¾ç½®ä¸‹é¢çš„ç¯å¢ƒå˜é‡ï¼ˆè€ƒè™‘åˆ°åç»­ä¹Ÿæœ‰å¯èƒ½å‡ºç°ç±»ä¼¼çš„è¿è¡Œæ—¶é—®é¢˜ï¼Œå»ºè®®åŠ å…¥å…¨å±€ <code>~/.bashrc</code> ä¸­ï¼‰ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">TRITON_LIBCUDA_PATH</span><span class="token operator">=</span>/usr/local/cuda-12/lib64/stubs</code></pre>
<p>ä¹‹åå°±å¯ä»¥ç»§ç»­ç¼–è¯‘äº†ï¼š</p>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> pybind11<br>pip <span class="token function">install</span> <span class="token parameter variable">-e</span> python</code></pre>
<p>ç¼–è¯‘å®‰è£…å®Œæ¯•åï¼Œå¯ä»¥éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼š</p>
<pre class="language-bash"><code class="language-bash">$ pip list <span class="token operator">|</span> <span class="token function">grep</span> triton<br>pytorch-triton                    <span class="token number">3.2</span>.0+git4b3bb1f8</code></pre>
<h3>Flash Attention</h3>
<p>è€ƒè™‘åˆ°å¾ˆå¤šå¤§æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨ <a href="https://github.com/Dao-AILab/flash-attention">flash attention</a> æ¥æé€Ÿä¼˜åŒ–ï¼Œè¿™é‡Œä¹Ÿå¯¹ flash attention è¿›è¡Œå®‰è£…ï¼Œæ ¹æ® <code>flash-attention</code> å®‰è£…è¯´æ˜ï¼Œæ ¹æ® CPU æ ¸å¿ƒæ•°ã€å†…å­˜è®¾å®šå¥½ä¸€ä¸ªä¿å®ˆçš„å¹¶è¡Œæ•°é‡ï¼ˆå› ä¸ºå†…å­˜è¿‡å°æ—¶ï¼Œè¿‡é«˜çš„å¹¶è¡Œæ•°ä¼šè®©è¯¥å®‰è£…è¿‡ç¨‹å¡æ­»ï¼‰å®‰è£…å³å¯ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token assign-left variable">MAX_JOBS</span><span class="token operator">=</span><span class="token number">4</span> pip <span class="token function">install</span> flash-attn --no-build-isolation</code></pre>
<p>ç”±äº Blackwell æš‚æ—¶ä¸æ”¯æŒ flash attention 3ï¼Œæ‰€ä»¥ä¹‹åä½¿ç”¨æ—¶ä¸€èˆ¬åªä½¿ç”¨ flash-attention 2ã€‚å¦‚æœé‡åˆ°äº† <a href="https://github.com/Dao-AILab/flash-attention/issues/1312">Dao-AILab/flash-attention#1312</a> ä¸­çš„ Operation Error: /usr/bin/ld: cannot find -lcuda é”™è¯¯ï¼Œå°±æ„å‘³ç€ä¹‹å‰çš„ <code>TRITON_LIBCUDA_PATH</code> æ²¡æœ‰è®¾ç½®å¥½ã€‚</p>
<h3>AutoAWQ</h3>
<p>Triton å®‰è£…å®Œæ¯•åï¼Œå®‰è£…è¯¥åŒ…ç›¸å¯¹å®¹æ˜“ï¼š</p>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> autoawq</code></pre>
<p>ä¹‹åå°±å¯ä»¥è¿è¡Œ AWQ é‡åŒ–å¤§æ¨¡å‹ã€‚</p>
<h3>Transformers</h3>
<p>è‡³æ­¤ï¼Œä½ åº”è¯¥èƒ½å¤Ÿä½¿ç”¨è¿™äº›å‰ç½®ä¾èµ–æ¥é«˜æ•ˆåœ°è¿è¡Œæœ¬åœ°å¤§æ¨¡å‹äº†ï¼é€šè¿‡å®‰è£… Huggingface çš„ <a href="https://github.com/huggingface/transformers">transformers</a>ï¼Œä½ å¤§æ¦‚èƒ½å¤Ÿåœ¨ 5070 Ti ä¸Šè·‘é€š DeepSeek-Distill-Qwen-14B-AWQï¼š</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> TextStreamer<br><span class="token keyword">import</span> torch<br><br>model_name <span class="token operator">=</span> <span class="token string">"casperhansen/deepseek-r1-distill-qwen-14b-awq"</span><br>prompt <span class="token operator">=</span> <span class="token string">"ä½ å¥½"</span><br><br>model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><br>    model_name<span class="token punctuation">,</span><br>    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span><br>    device_map<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span><br>    attn_implementation<span class="token operator">=</span><span class="token string">"flash_attention_2"</span><span class="token punctuation">,</span><br><span class="token punctuation">)</span><br>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><br>    model_name<span class="token punctuation">,</span><br><span class="token punctuation">)</span><br><br>streamer <span class="token operator">=</span> TextStreamer<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span><br>text <span class="token operator">=</span> prompt <span class="token operator">+</span> <span class="token string">"&lt;think>"</span><br>inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span><br>outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><br>    <span class="token operator">**</span>inputs<span class="token punctuation">,</span><br>    max_new_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span><br>    streamer<span class="token operator">=</span>streamer<span class="token punctuation">,</span><br>    do_sample<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><br><span class="token punctuation">)</span></code></pre>
<p>ç”±äºå›½å†…è®¿é—® Huggingface è¾ƒä¸ºç¼“æ…¢ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘ä½¿ç”¨<a href="https://hf-mirror.com/">é•œåƒåœ°å€</a>è¿è¡Œè¯¥è„šæœ¬ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com python start_llm.py</code></pre>
<p>æœ€åå¯ä»¥è¾“å‡ºï¼š</p>
<pre class="language-plain"><code class="language-plain">&lt;ï½œbeginâ–ofâ–sentenceï½œ>ä½ å¥½&lt;think><br><br>&lt;/think><br><br>ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š&lt;ï½œendâ–ofâ–sentenceï½œ></code></pre>
<p>é€šè¿‡ <code>watch nvidia-smi</code> ç›‘æµ‹å ç”¨å¦‚ä¸‹ï¼š</p>
<pre class="language-text"><code class="language-text">Sun Mar 23 23:50:15 2025<br>+-----------------------------------------------------------------------------------------+<br>| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |<br>|-----------------------------------------+------------------------+----------------------+<br>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |<br>|                                         |                        |               MIG M. |<br>|=========================================+========================+======================|<br>|   0  NVIDIA GeForce RTX 5070 Ti     Off |   00000000:01:00.0  On |                  N/A |<br>| 30%   53C    P1            300W /  300W |   10102MiB /  16303MiB |     99%      Default |<br>|                                         |                        |                  N/A |<br>+-----------------------------------------+------------------------+----------------------+<br><br>+-----------------------------------------------------------------------------------------+<br>| Processes:                                                                              |<br>|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |<br>|        ID   ID                                                               Usage      |<br>|=========================================================================================|<br>|    0   N/A  N/A           10730      C   python                                 9964MiB |<br>+-----------------------------------------------------------------------------------------+</code></pre>
<h3>VLLM</h3>
<p><a href="https://github.com/vllm-project/vllm">VLLM</a> å¸¸è¢«ç”¨æ¥åšå¤§æ¨¡å‹æ¨ç†éƒ¨ç½²ï¼ŒæŒ‰ç…§ <a href="https://github.com/vllm-project/vllm/issues/14452">å®éªŒæ€§è¯´æ˜</a>ï¼Œè¿›è¡Œå¦‚ä¸‹æ­¥éª¤ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/vllm-project/vllm.git <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> vllm<br>python use_existing_torch.py<br>pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements/build.txt<br>pip <span class="token function">install</span> setuptools_scm</code></pre>
<p>ä¹‹åï¼Œéœ€è¦å¯¹ä¸€äº›ç¯å¢ƒå˜é‡è¿›è¡Œä¿®æ”¹/æˆ–è€…åœ¨ <code>vllm/setup.py</code> ä¸­å¯¹CMAKEæ·»åŠ ç¯å¢ƒå˜é‡ï¼ˆå¦åˆ™ä¼šå‡ºç° Caffe2 æ‰¾ä¸åˆ° CUDA çš„æŠ¥é”™ï¼‰ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_TOOLKIT_ROOT_DIR</span><span class="token operator">=</span>/usr/local/cuda<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_INCLUDE_DIRS</span><span class="token operator">=</span>/usr/local/cuda/include</code></pre>
<p>ä¼¼ä¹ CUDA 12.8 æš‚æ—¶æ²¡æœ‰å¯¹ NVTX3 çš„æ”¯æŒï¼ˆä¸è¿›è¡Œè¿™ä¸€æ­¥åé¢ä¼šå‡ºç°<a href="https://discuss.pytorch.org/t/libtorch-cannot-find-nvtx3-find-old-nvtx-instead/213641">é“¾æ¥é”™è¯¯</a>ï¼‰ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨ä¸‹è½½ NVTX åº“åˆ°ä¸€ä¸ªè·¯å¾„ <code>&lt;path&gt;</code> ä¸Šï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/NVIDIA/NVTX.git <span class="token parameter variable">--depth</span> <span class="token number">1</span></code></pre>
<p>ç„¶åå¯¹ <code>~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake</code>ï¼ˆPython ç¯å¢ƒä¸­çš„ torch åŒ…çš„ä¸€ä¸ªæ–‡ä»¶ï¼‰ ä¸­çš„ä¸‹é¢æ³¨é‡Šå¤„è¿›è¡Œæ›¿æ¢ï¼š</p>
<pre class="language-cmake"><code class="language-cmake"><span class="token comment"># nvToolsExt</span><br><span class="token comment"># if(USE_SYSTEM_NVTX)</span><br><span class="token comment">#   find_path(nvtx3_dir NAMES nvtx3 PATHS ${CUDA_INCLUDE_DIRS})</span><br><span class="token comment"># else()</span><br><span class="token comment">#   find_path(nvtx3_dir NAMES nvtx3 PATHS "${PROJECT_SOURCE_DIR}/third_party/NVTX/c/include" NO_DEFAULT_PATH)</span><br><span class="token comment"># endif()</span><br><span class="token keyword">find_path</span><span class="token punctuation">(</span>nvtx3_dir NAMES nvtx3 PATHS <span class="token string">"&lt;path>/NVTX/c/include"</span><span class="token punctuation">)</span> <span class="token comment"># use custom nvtx3, replace &lt;path> to your path</span></code></pre>
<p>ç»§ç»­æŒ‰ç…§è¯´æ˜ï¼Œå°±å¯ä»¥æ­£ç¡®ç¼–è¯‘ vllm äº†ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token assign-left variable">MAX_JOBS</span><span class="token operator">=</span><span class="token number">4</span> <span class="token assign-left variable">VLLM_FLASH_ATTN_VERSION</span><span class="token operator">=</span><span class="token number">2</span> python setup.py develop</code></pre>
<p>ä¹‹åï¼Œè¿˜è¦åšä¸€äº›å–„åå·¥ä½œï¼Œæ¯”å¦‚ <code>triton</code> æ‰¾ä¸åˆ°äº†ï¼ˆImportError: cannot import name 'Config' from 'triton' (unknown location)ï¼‰ï¼Œéœ€è¦è¿”å›ä¹‹å‰çš„ <code>triton</code> ä»“åº“é‡æ–°å®‰è£…ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">cd</span> triton <span class="token operator">&amp;&amp;</span> pip <span class="token function">install</span> <span class="token parameter variable">-e</span> python</code></pre>
<p>ä»¥åŠ ImportError: Numba needs NumPy 2.0 or less. Got NumPy 2.2.ï¼Œå®‰è£… numpy 2.0ï¼š</p>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token assign-left variable">numpy</span><span class="token operator">==</span><span class="token number">2.0</span></code></pre>
<p>ä¹‹åå°±å¯ä»¥é¡ºåˆ©åœ°éƒ¨ç½² VLLM äº†ï¼ä¸ºäº†é¿å…æ˜¾å­˜æº¢å‡ºï¼Œéœ€è¦è°ƒæ•´é»˜è®¤å‚æ•°ï¼Œæ¯”å¦‚æŒ‰ç…§ä¸‹é¢çš„å‘½ä»¤å°±å¯ä»¥æˆåŠŸéƒ¨ç½²ï¼š</p>
<pre class="language-bash"><code class="language-bash">vllm serve /home/<span class="token operator">&lt;</span>username<span class="token operator">></span>/.cache/huggingface/hub/models--casperhansen--deepseek-r1-distill-qwen-14b-awq/snapshots/bc43ec1bbf08de53452630806d5989208b4186db <span class="token parameter variable">--max_num_seqs</span> <span class="token number">2</span> <span class="token parameter variable">--gpu_memory_utilization</span> <span class="token number">0.9</span> <span class="token parameter variable">--max_model_len</span> <span class="token number">2048</span> <span class="token parameter variable">--port</span> <span class="token number">3003</span></code></pre>
<p>å…¶ä¸­ <code>max_num_seqs</code> ä»£è¡¨åŒæ—¶å¯ä»¥å¤„ç†çš„æ•°é‡ï¼Œ<code>max_model_len</code> æ˜¯æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ<code>port</code> æ˜¯æš´éœ²çš„ç«¯å£å·ï¼Œä¹‹åå°±å¯ä»¥ä½¿ç”¨ OpenAI å…¼å®¹æ ‡å‡†è°ƒé€šæ¨¡å‹ã€‚</p>
<details>
<summary>
æ‰‹åŠ¨æ¶ˆé™¤ egg è­¦å‘Š
</summary>
<p>å°†ä¸‹é¢çš„ä»£ç æ®µä¿å­˜ä¸º <code>requirements.txt</code>ï¼Œç„¶åè¿è¡Œ <code>pip install --force-reinstall -r requirements.txt --no-deps</code> é‡æ–°å®‰è£…ã€‚</p>
<pre class="language-text"><code class="language-text">python_json_logger==3.3.0<br>distro==1.9.0<br>lm_format_enforcer==0.10.11<br>dnspython==2.7.0<br>lark==1.2.2<br>httpx==0.28.1<br>openai==1.68.2<br>msgspec==0.19.0<br>cloudpickle==3.1.1<br>uvicorn==0.34.0<br>depyf==0.18.0<br>nest_asyncio==1.6.0<br>python_dotenv==1.0.1<br>numpy==1.26.4<br>anyio==4.9.0<br>transformers==4.50.0<br>tiktoken==0.9.0<br>xgrammar==0.1.16<br>opencv_python_headless==4.11.0.86<br>websockets==15.0.1<br>httpcore==1.0.7<br>partial_json_parser==0.2.1.1.post5<br>scipy==1.15.2<br>h11==0.14.0<br>pyzmq==26.3.0<br>shellingham==1.5.4<br>ray==2.44.0<br>outlines==0.1.11<br>python_multipart==0.0.20<br>sentencepiece==0.2.0<br>watchfiles==1.0.4<br>compressed_tensors==0.9.2<br>sniffio==1.3.1<br>rich_toolkit==0.13.2<br>mistral_common==1.5.4<br>jiter==0.9.0<br>fastapi==0.115.12<br>airportsdata==20250224<br>interegular==0.3.3<br>uvloop==0.21.0<br>llvmlite==0.43.0<br>numba==0.60.0<br>outlines_core==0.1.26<br>httptools==0.6.4<br>fastapi_cli==0.0.7<br>starlette==0.46.1<br>pycountry==24.6.1<br>diskcache==5.6.3<br>typer==0.15.2<br>astor==0.8.1<br>blake3==1.0.4<br>email_validator==2.2.0<br>llguidance==0.7.9<br>cachetools==6.0.0b1<br>prometheus_fastapi_instrumentator==7.1.0<br>gguf==0.10.0<br>prometheus_client==0.21.1<br>fastrlock==0.8.3<br>cupy_cuda12x==13.4.1</code></pre>
</details>
<h3>DeepSpeed</h3>
<p><a href="https://github.com/deepspeedai/DeepSpeed">DeepSpeed</a> å¯ä»¥è¢«ç”¨æ¥åˆ†å¸ƒå¼æ¨ç†ä¸è®­ç»ƒã€‚é¦–å…ˆï¼Œå…‹éš†å­˜å‚¨åº“ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/deepspeedai/DeepSpeed.git <span class="token parameter variable">--depth</span> <span class="token number">1</span><br><span class="token builtin class-name">cd</span> DeepSpeed</code></pre>
<p>å°è¯•ç›´æ¥å®‰è£…ï¼š</p>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> <span class="token builtin class-name">.</span></code></pre>
<p>å¯ä»¥ä½¿ç”¨ <code>ds_report</code> æ¥çœ‹æœ‰å“ªäº›æŠ¥é”™ï¼Œä¸€èˆ¬è€Œè¨€éœ€è¦åšä¸‹é¢çš„é¢å¤–æ­¥éª¤ï¼š</p>
<ol>
<li>å¯¹äº <code>[WARNING]  gds: please install the libaio-dev package with apt</code>ï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å®‰è£…è¿™ä¸ªåº“ï¼š</li>
</ol>
<pre class="language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> libaio-dev</code></pre>
<ol start="2">
<li>ç„¶åå¯èƒ½è¿˜ä¼šæœ‰ <code>ld</code> é“¾æ¥é”™è¯¯ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œéœ€è¦ä¹ŸæŒ‡å®šä¸º GCC 13 ç‰ˆæœ¬ï¼š</li>
</ol>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span>/usr/bin/gcc-13<br><span class="token builtin class-name">export</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span>/usr/bin/g++-13</code></pre>
<ol start="3">
<li>éœ€è¦å…‹éš† <a href="https://github.com/NVIDIA/cutlass">cutlass</a> åˆ° <code>&lt;path&gt;</code>ï¼Œå¹¶æŒ‡å®š <code>CUTLASS_PATH</code>ï¼š</li>
</ol>
<pre class="language-bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUTLASS_PATH</span><span class="token operator">=</span><span class="token operator">&lt;</span>path<span class="token operator">></span>/cutlass</code></pre>
<ol start="4">
<li>æ‰¾ä¸åˆ° <code>dskernels</code>ï¼Œéœ€è¦å®‰è£…è¿™ä¸ªåŒ…ï¼š</li>
</ol>
<pre class="language-bash"><code class="language-bash">pip <span class="token function">install</span> deepspeed-kernels</code></pre>
<p>ä¹‹åå°±å¯ä»¥è¿›è¡Œç¼–è¯‘å®‰è£…æ‰€æœ‰å­åŒ…ï¼Œå¯èƒ½æœ‰äº›æ— æ³•ç¼–è¯‘å¯ä»¥å¿½ç•¥ï¼š</p>
<pre class="language-bash"><code class="language-bash"><span class="token assign-left variable">DS_BUILD_OPS</span><span class="token operator">=</span><span class="token number">1</span> pip <span class="token function">install</span> <span class="token builtin class-name">.</span> --global-option<span class="token operator">=</span><span class="token string">"build_ext"</span> --global-option<span class="token operator">=</span><span class="token string">"-j8"</span></code></pre>
<p>å®‰è£…å®Œæ¯•åä½¿ç”¨ <code>ds_report</code> å‘½ä»¤éªŒè¯ï¼š</p>
<pre class="language-text"><code class="language-text">--------------------------------------------------<br>DeepSpeed C++/CUDA extension op report<br>--------------------------------------------------<br>NOTE: Ops not installed will be just-in-time (JIT) compiled at<br>      runtime if needed. Op compatibility means that your system<br>      meet the required dependencies to JIT install the op.<br>--------------------------------------------------<br>JIT compiled ops requires ninja<br>ninja .................. [OKAY]<br>--------------------------------------------------<br>op name ................ installed .. compatible<br>--------------------------------------------------<br>async_io ............... [YES] ...... [OKAY]<br>fused_adam ............. [YES] ...... [OKAY]<br>cpu_adam ............... [YES] ...... [OKAY]<br>cpu_adagrad ............ [YES] ...... [OKAY]<br>cpu_lion ............... [YES] ...... [OKAY]<br>evoformer_attn ......... [YES] ...... [OKAY]<br> [WARNING]  FP Quantizer is using an untested triton version (3.3.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels<br>fp_quantizer ........... [NO] ....... [NO]<br>fused_lamb ............. [YES] ...... [OKAY]<br>fused_lion ............. [YES] ...... [OKAY]<br>gds .................... [YES] ...... [OKAY]<br>transformer_inference .. [YES] ...... [OKAY]<br>inference_core_ops ..... [YES] ...... [OKAY]<br>cutlass_ops ............ [YES] ...... [OKAY]<br>quantizer .............. [YES] ...... [OKAY]<br>ragged_device_ops ...... [YES] ...... [OKAY]<br>ragged_ops ............. [YES] ...... [OKAY]<br>random_ltd ............. [YES] ...... [OKAY]<br> [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.7<br> [WARNING]  using untested triton version (3.3.0), only 1.0.0 is known to be compatible<br>sparse_attn ............ [NO] ....... [NO]<br>spatial_inference ...... [YES] ...... [OKAY]<br>transformer ............ [YES] ...... [OKAY]<br>stochastic_transformer . [YES] ...... [OKAY]</code></pre>
<p>ä¹‹åä½ å¯èƒ½éœ€è¦é‡æ–°å®‰è£… tritonã€‚</p>

</div>

    <footer>
        <div class="widthlimit">
            <p>ç‰ˆæƒæ‰€æœ‰ Â© Log Creative 2012-2025ï¼Œä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚</p>
            <p>é™¤æœ‰å¼€æºåè®®å£°æ˜çš„ï¼Œæœªç»è®¸å¯ï¼Œä¸å¯ç”¨äºå•†ä¸šç”¨é€”ã€‚</p>
            <p>Powered by <a href="https://www.11ty.dev/" target="_blank">Eleventy</a>&nbsp;|&nbsp;<a href="https://beian.miit.gov.cn" target="_blank">æ²ªICPå¤‡2021032524å·</a></p>
        </div>
    </footer>
</body>
</html>