<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="/favicon.ico">
    <title>PGFPlotsEdt 大模型应用混合部署方式</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://unpkg.com/prismjs@1.20.0/themes/prism.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/css/lcard.css"></head>
<body>
    <div id="navigation">
        <div class="widthlimit">
        <a href="/"><img src="/assets/images/LC.gif" height="64px"></a>
        <ul><li>
                    <a href="/">主页</a>
                </li><li>
                    <a href="/blogs">博客</a>
                </li><li>
                    <a href="/archive">存储库</a>
                </li><li>
                    <a href="/notes">课程记录</a>
                </li><li>
                    <a href="/about">关于</a>
                </li></ul>
        </div>
    </div>

    
<div id="main" class="widthlimit">

    <h1>PGFPlotsEdt 大模型应用混合部署方式</h1>

    <h2>探讨通过传统架构、无服务器、端侧推理三种模式以尽可能低的成本提供大模型应用服务。</h2>

    <h2>2025-10-12</h2>

    <a href="https://cloud.tencent.com/act/cps/redirect?redirect=6150&cps_key=3b86b83ec1516f560c5dc351fe35065b&from=console"><img src="/assets/images/rhino-design-1200x90x2.png" alt="【腾讯云】2核2G云服务器新老同享 99元/年，续费同价，云服务器3年机/5年机限时抢购，低至 2.5折" style="max-width:100%;cursor:pointer;"></img></a>

    <h3>缘起：智能编辑</h3>
<p><a href="https://github.com/LogCreative/PGFPlotsEdt">PGFPlotsEdt</a> 是一款 \(\rm\LaTeX\) PGFPlots 代码式统计绘图在线绘制工具，旨以模块化的图形式菜单引导用户快速调配出统计图代码，并通过快速编译机制尽快得到预览统计图以供调试。</p>
<p>尽管图形化菜单为用户提供了便捷的操作方式，但受限于功能适配的复杂性（<a href="https://mirrors.sjtug.sjtu.edu.cn/CTAN/graphics/pgf/contrib/pgfplots/doc/pgfplots.pdf">PGFPlots 文档</a> 长达 500 多页，PGFPlotsEdt 的菜单仅覆盖了其中一部分功能），可通过菜单生成的代码空间相对有限。对于开发者而言，持续将更多功能集成到菜单中不仅工作量巨大，还可能导致界面臃肿、影响易用性。因此，PGFPlotsEdt 也提供了“手动编辑”按钮，允许用户直接灵活地修改代码。然而，为了满足更复杂的定制需求，用户往往需要频繁查阅宏包文档，这使得编写个性化统计图代码依然存在一定门槛和难度。</p>
<p>幸运的是，随着大模型（LLM）技术的持续进步，代码生成迎来了全新的解决方案。代码本身具备高度结构化和明确语义，使其成为用户与大模型高效交互的理想载体。当前主流大模型已能够生成高质量、可编译的代码，基本满足用户的实际需求。因此，PGFPlotsEdt 在 <a href="https://github.com/LogCreative/PGFPlotsEdt/releases/tag/3.5">3.5 版本</a>（2024-05-05）首次推出了“智能编辑”功能，允许用户通过自然语言提示，引导大模型自动修改和完善代码。经过多个版本的迭代升级，如今的“智能编辑”功能已融合检索增强生成（RAG）技术，能够结合相关文档内容，进一步提升大模型生成 PGFPlots 代码的准确性和实用性。</p>
<h3>原理：RAG 工作流</h3>
<p><a href="https://github.com/LogCreative/PGFPlotsEdt/releases/tag/4.5">PGFPlotsEdt 4.5 版本</a> 具体的 RAG 工作流大致如下：</p>
<pre class="mermaid">graph LR
    A1[/User Query/] --&#x3E; B;
    A2[/Current Code/] --&#x3E; D;

    B{Transformation?};

    B -- No (Direct Input) --&#x3E; C1[Final Query];

    B -- Yes (Transform) --&#x3E; T[\LLM/];

    T --&#x3E; |Transformed Query| C1;

    C1 --&#x3E;|Text Input| C2[/Embedding Model\];
    C2 --&#x3E; |Query Vector| E[(Vector DB)];
    E --&#x3E; |Vector Search| F@{ shape: docs, label: &#x22;Related chunks&#x22;};

    F --&#x3E; D[\LLM/];

    C1 --&#x3E; D;

    D --&#x3E; G[Final Code Output];

    style A1 fill:#f9f,stroke:#333,stroke-width:2px
    style A2 fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333
    style T fill:#9f9,stroke:#333
    style C1 fill:#f0c0ff,stroke:#333
    style C2 fill:#9f9,stroke:#333
    style E fill:#fcc,stroke:#333
    style F fill:#ffffcc,stroke:#333
    style D fill:#9f9,stroke:#333
    style G fill:#f9f,stroke:#333,stroke-width:2px
</pre>
<p>RAG 部分的源代码实现于<a href="https://github.com/LogCreative/PGFPlotsEdt/blob/1fc7f09295525ee0b1d540e31672feec74a2ec7e/ppedt_server_llm.py#L147-L204">此</a>，简单来讲：</p>
<ol>
<li><strong>问题改写</strong> 相比于直接使用 LLM，为了实现与文档最好的匹配效果，这里先对用户问题进行改写（Query Transformation），由于后续的文档库片段来自 PGFPlots 英文 \(\rm\LaTeX\) 源代码，所以这里会要求大模型先翻译成英文，并且将一些 Unicode 数学符号转换成 \(\rm\LaTeX\) 命令，如果不包含非 ASCII 字符则不需要转换，所以如果进入问题改写分支就会一些花费额外的时间（+可选的较长首token时间）；</li>
<li><strong>检索阶段</strong> 然后进入检索阶段，这里统一使用 <a href="https://huggingface.co/BAAI/bge-small-en-v1.5">BAAI/bge-small-en-v1.5</a> 这个只有 33.4M 参数量的模型生成 384 维嵌入向量，通过余弦相似度在向量数据库中匹配文档块，这里的文档切分算法主要以分节为界，每个块不超过 500 字符（后续应该可以通过切分更为完整的 \(\rm\LaTeX\) 环境块进行优化），捞出 Top 3 的文档块（必要时移除余弦相似度小于 0.75 的块），由于文档空间并不大（共 5054 个嵌入向量），所以这一步目前的时间和空间开销都不是很大（+较短首token时间）；</li>
<li><strong>生成阶段</strong> 最后进入生成阶段，将改写问题（或者是用户原始问题提升内容忠实度？但是这里用到的 Llama 3 对于英文更强一些，Llama 3.1 对多语种的支持会更好，当然跟改写阶段用的是同一个大模型罢了）、文档块文件名和内容（遵循 <a href="https://www.llamaindex.ai/">llama-index</a> 的通用做法）、当前的代码，要求 LLM 根据用户问题对代码进行改善并不进行任何解释。这里使用 <a href="https://llama.meta.com/llama3/">Llama 3</a>-8B 大模型（后来的 Llama 3.1-8B 测试后并没有达到这个场景的预期效果，笔者认为是在同参数量规模下塞入更多的功能可能会减少部分功能的能力），因为参数量足够少并且 <a href="https://llm.mlc.ai/">MLC LLM</a> 框架、Cloudflare Serverless <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a> 适配较好，小参数量本地推理开销少；如果是主站会使用 <a href="https://github.com/zai-org/GLM-4.5">GLM-4.6</a>，使用现代化一些的大模型可以有更好的效果。在这个阶段，大模型的流式输出结果会经过代码过滤器传到前端输出。</li>
</ol>
<p>该 RAG 流程相较于直接询问 LLM，可以提升生成内容的准确性，补充大模型在 PGFPlots 预训练语料的不足；相较于通过微调（fine-tuning）LLM 的方式，可以更为方便且更模块化地提升生成的内容的效果，也可以有更多公共资源可以选择（微调后的模型一般需要自行部署，而通用大模型通常公网有很多的服务商进行服务）。当然 PGFPlotsEdt 也提供了实验性的微调大模型 <a href="https://huggingface.co/LogCreative/Llama-3-8B-Instruct-pgfplots-finetune-q4f16_1-MLC">Llama-3-8B-Instruct-pgfplots-finetune</a>，可以在本地 MLC LLM 框架中<a href="https://github.com/LogCreative/PGFPlotsEdt/blob/1fc7f09295525ee0b1d540e31672feec74a2ec7e/ppedt_server_llm.py#L38-L39">加载试用</a>。</p>
<h3>部署：减少开销</h3>
<p>但是，转型到大模型应用后，由于大模型需要大量的算力，对于一个个人非商业项目而言，这种算力带来的成本是无法忽略的。因此，PGFPlotsEdt 将尽可能使用低价甚至免费的多种部署方式，提供优质服务的同时减少不必要的个人开销。</p>
<p>对于 PGFPlotsEdt 这套 RAG 工作流而言，需要三个高耗费资源：向量数据库、嵌入模型服务和大模型服务。对于这些服务，PGFPlotsEdt 对于不同的访问途径分为三套不同的资源及系统架构进行服务：</p>
<pre class="mermaid">block
  columns 3
  block:group1:1
    columns 3
    pg[(&#x22;PostgreSQL&#x22;)] embed[/&#x22;text-embeddings-inference&#x22;\] glm[\&#x22;Zhipu API&#x22;/]
  end
  block:group2:1
    columns 3
    vec[(&#x22;Vectorize&#x22;)] cfe[/&#x22;Cloudflare Embedding&#x22;\] cfl[\&#x22;Cloudflare Llama 3&#x22;/]
  end
  block:group3:1
    columns 3
    inmem[(&#x22;in-mem&#x22;)] he[/&#x22;Huggingface Embedding Model&#x22;\] mlc[\&#x22;MLC LLM&#x22;/]
  end
  
  li[&#x22;llama-index&#x22;] wai[&#x22;Workers AI&#x22;] li2[&#x22;llama-index&#x22;]

  gunicorn Workers flask
  
  nginx ghp[&#x22;GitHub Pages&#x22;] ls[&#x22;live server&#x22;]

  b[&#x22;logcreative.tech&#x22;] c[&#x22;logcreative.github.io&#x22;] d[&#x22;local server&#x22;]
  a[&#x22;PGFPlotsEdt frontend&#x22;]:3
</pre>
<h4>国内：传统架构</h4>
<p>国内站 <a href="https://logcreative.tech/PGFPlotsEdt">https://logcreative.tech/PGFPlotsEdt</a> 使用传统的服务架构。</p>
<p><strong>基础架构</strong> 使用 2 台腾讯云轻量服务器、云数据库 PostgreSQL 作为基础架构。PostgreSQL 启用 <a href="https://github.com/pgvector/pgvector">pgvector</a> 插件后可以作为向量数据库使用；基于 Rust 的 <a href="https://github.com/huggingface/text-embeddings-inference">text-embeddings-inference</a> 嵌入模型推理工具可以更好地释放 CPU 推理性能，并提供 OpenAI 兼容接口；使用智谱 <a href="https://github.com/zai-org/GLM-4.5">GLM-4.6</a> 模型 OpenAI 兼容接口作为<a href="https://docs.bigmodel.cn/cn/guide/platform/filing">国内合规大模型服务</a>。</p>
<p><strong>智能体框架</strong> 使用 <a href="https://www.llamaindex.ai/">llama-index</a> 这个更偏向于 RAG 的智能体框架，可以通过 <a href="https://developers.llamaindex.ai/python/examples/vector_stores/postgres/">PGVectorStore</a> 适配 PostgreSQL 数据库，通过 OpenAILikeEmbedding 连接 OpenAI 兼容接口的嵌入模型，并通过 OpenAILike 连接 OpenAI 兼容接口的大模型。（不能省略 Like，否则只能连接 OpenAI 官方模型）</p>
<p><strong>中间件</strong> 使用 <a href="https://gunicorn.org/">gunicorn</a> 对 <a href="https://flask.org.cn/">Flask</a> 应用进行生产级优化，该框架只能在 Linux 或 MacOS 上使用；当然现在更流行的 Python 中间件是 <a href="https://uvicorn.dev/">uvicorn</a>，后续可能会迁移到该框架上。该层还会通过预编译头缓存加速编译准备阶段的时间，基于相同的初始 \(\rm\LaTeX\) 模板种子，用户的起始编译代码编译头大致相同或有缓存，对于多用户的服务会有更快的最初编译响应。</p>
<p><strong>负载均衡</strong> 这里使用 <a href="https://nginx.org/">nginx</a> 作为负载均衡中间件，前端直接使用 nginx 进行静态文件服务，后端使用 <code>proxy_pass</code> 代理到对应的服务上。这里对 \(\rm\LaTeX\) 编译服务和大模型服务的处理是不同的，一个示例的配置文件片段如下：</p>
<pre class="language-nginx"><code class="language-nginx"><span class="token directive"><span class="token keyword">location</span> /compile</span> <span class="token punctuation">{</span><br>  <span class="token directive"><span class="token keyword">proxy_pass</span> http://pgfplotsedt-backend/compile</span><span class="token punctuation">;</span><br><span class="token punctuation">}</span><br><br><span class="token directive"><span class="token keyword">location</span> /llm</span> <span class="token punctuation">{</span><br>  <span class="token directive"><span class="token keyword">proxy_buffering</span> <span class="token boolean">off</span></span><span class="token punctuation">;</span> <span class="token comment"># to make it streamable</span><br>  <span class="token directive"><span class="token keyword">proxy_read_timeout</span> <span class="token number">3m</span></span><span class="token punctuation">;</span><br>  <span class="token directive"><span class="token keyword">proxy_pass</span> http://localhost:5678/llm</span><span class="token punctuation">;</span><br><span class="token punctuation">}</span></code></pre>
<ul>
<li><code>/compile</code> 对应的编译服务不需要流式返回，但是编译需要服务器计算资源，这里采用两个云服务器通过 <a href="https://docs.docker.com/compose/">docker compose</a> 同时部署上述的服务，暴露出端口后，nginx 进行负载均衡；</li>
<li><code>/llm</code> 对应的大模型服务需要流式返回数据，Flask <a href="https://flask.org.cn/en/stable/patterns/streaming/">文档</a>中也提示过这样的话语：但请注意，某些 WSGI 中间件可能会破坏流式传输。这里也不例外，对于 nginx 来说，需要关闭 <a href="https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering"><code>proxy_buffering</code></a> 功能，否则 nginx 会尝试缓存这些流式响应然后再一起返回，减少了响应性。nginx 这一层会默认将响应类型变更为 <code>Application/octet-stream</code>，保险起见并为了方便本地调试，Flask 层也应当将流式的部分<a href="https://github.com/LogCreative/PGFPlotsEdt/blob/1fc7f09295525ee0b1d540e31672feec74a2ec7e/ppedt_server.py#L254">响应类型变更</a>为 <code>Application/octet-stream</code>。</li>
</ul>
<p>最终通过 <a href="https://logcreative.tech">logcreative.tech</a> 这个域名服务出去。这套传统的部署方式符合大多数软件服务的成熟服务方式，也符合相关监管要求。</p>
<h4>国际：无服务器</h4>
<p>国际站 <a href="https://logcreative.github.io/PGFPlotsEdt">https://logcreative.github.io/PGFPlotsEdt</a> 使用无服务器（serverless）架构。</p>
<p><strong>基础架构</strong> 使用 <a href="https://developers.cloudflare.com/workers/">Cloudflare Workers</a>、<a href="https://developers.cloudflare.com/vectorize/">Cloudflare Vectorize</a> 作为基础架构。Vectorize 提供的向量数据库可以很好地集成 Cloudflare Workers 中，但是导入数据需要使用 <a href="https://developers.cloudflare.com/workers/wrangler/commands/#vectorize">wrangler 命令</a> 进行，PGFPlotsEdt 使用上文通过 llama-index 导入 PostgreSQL 中的向量化数据处理后进行导入，详见<a href="https://github.com/LogCreative/PGFPlotsEdt/blob/1fc7f09295525ee0b1d540e31672feec74a2ec7e/deploy/init_kb_serverless.py">导入脚本</a>；嵌入模型和大模型均由 <a href="https://developers.cloudflare.com/workers-ai/">Workers AI</a> 提供，每日有 10k 的免费神经元限额。</p>
<p><strong>智能体框架</strong> 智能体框架直接采用适用于 Cloudflare 无服务器环境的 Workers AI 原生重写。</p>
<p>这种方式部署出来的服务，会有跨域问题（CORS），即由于域名不同（实际上该问题需要协议、域名、端口都相同）， PGFPlotsEdt 前端无法正常访问 Cloudflare 服务，这里通过对后端响应添加相关响应头解决。（如果是传统架构，还可以使用 nginx 解决这个问题。）</p>
<pre class="language-js"><code class="language-js"><span class="token keyword">export</span> <span class="token keyword">default</span> <span class="token punctuation">{</span><br>  <span class="token keyword">async</span> <span class="token function">fetch</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span> env<span class="token punctuation">,</span> ctx</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token keyword">const</span> corsHeaders <span class="token operator">=</span> <span class="token punctuation">{</span><br>      <span class="token string-property property">'Access-Control-Allow-Origin'</span><span class="token operator">:</span> <span class="token string">'*'</span><span class="token punctuation">,</span><br>      <span class="token string-property property">'Access-Control-Allow-Methods'</span><span class="token operator">:</span> <span class="token string">'GET,HEAD,POST,OPTIONS'</span><span class="token punctuation">,</span><br>      <span class="token string-property property">'Access-Control-Allow-Headers'</span><span class="token operator">:</span> <span class="token string">'Content-Type,Authorization'</span><br>    <span class="token punctuation">}</span><span class="token punctuation">;</span><br>    <span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>method <span class="token operator">===</span> <span class="token string">'OPTIONS'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>      <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Response</span><span class="token punctuation">(</span><span class="token keyword">null</span><span class="token punctuation">,</span> <span class="token punctuation">{</span> <span class="token literal-property property">status</span><span class="token operator">:</span> <span class="token number">204</span><span class="token punctuation">,</span> <span class="token literal-property property">headers</span><span class="token operator">:</span> corsHeaders <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span><br>    <span class="token keyword">if</span> <span class="token punctuation">(</span>request<span class="token punctuation">.</span>method <span class="token operator">===</span> <span class="token string">"POST"</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>      <span class="token comment">// ...</span><br>      <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Response</span><span class="token punctuation">(</span>stream<span class="token punctuation">,</span> <span class="token punctuation">{</span><br>        <span class="token literal-property property">status</span><span class="token operator">:</span> <span class="token number">200</span><span class="token punctuation">,</span><br>        <span class="token literal-property property">headers</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>          <span class="token operator">...</span>corsHeaders<span class="token punctuation">,</span><br>          <span class="token string-property property">'Content-Type'</span><span class="token operator">:</span> <span class="token string">'application/octet-stream'</span><br>        <span class="token punctuation">}</span><br>      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span><br>      <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Response</span><span class="token punctuation">(</span><span class="token string">"..."</span><span class="token punctuation">,</span> <span class="token punctuation">{</span> <span class="token literal-property property">status</span><span class="token operator">:</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token literal-property property">headers</span><span class="token operator">:</span> corsHeaders <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span><br>  <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre>
<p><strong>部署平台</strong> 直接使用 <a href="https://docs.github.com/en/pages">GitHub Pages</a> 作为前端部署平台，编译服务直接使用 <a href="https://latexonline.cc">LaTeXOnline</a> 提供 \(\rm\LaTeX\) 的编译服务，由于没有编译相关的优化，编译上可能会增加一倍的时间，具体的评测信息可以参见 <a href="https://github.com/LogCreative/pgfplots-benchmark">pgfplots-benchmark</a>。</p>
<p>最终就可以直接通过 <a href="https://logcreative.github.io">logcreative.github.io</a> 部署出去。这套无服务器的部署方案可以尽可能使用国际网络生态中的免费或低价资源进行部署，减少了额外的部署开销。</p>
<h4>本地：端侧推理</h4>
<p>本地站 <a href="http://localhost:5678">http://localhost:5678</a> 使用简单的 Flask 应用架构。</p>
<p><strong>基础架构</strong> 只使用用户自己的计算机。基于 <a href="https://developers.llamaindex.ai/">llama-index</a> 提供的 SimpleVectorStore 内存向量数据库每次启动时进行向量化索引（由于文件块不多，所以这种开销是可以接受的）；嵌入模型使用 <a href="https://github.com/UKPLab/sentence-transformers">SentenceTransfomer</a> 基于 CPU 进行推理；Llama3-8b 大模型使用 <a href="https://llm.mlc.ai/">MLC LLM</a> 框架在各种异构 GPU 上进行大模型推理。</p>
<p><strong>智能体框架</strong> 使用 llama-index 作为智能体框架，但是需要连接本地的向量库、嵌入模型、大模型。其中大模型的部分由于其并没有提供对 MLC LLM 框架的原生支持，所以需要以 <a href="https://developers.llamaindex.ai/python/framework/module_guides/models/llms/usage_custom/">CustomLLM</a> 基类自行实现：</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> mlc_llm <span class="token keyword">import</span> MLCEngine<br><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core<span class="token punctuation">.</span>llms <span class="token keyword">import</span> <span class="token punctuation">(</span><br>  CustomLLM<span class="token punctuation">,</span><br>  CompletionResponse<span class="token punctuation">,</span><br>  CompletionResponseGen<span class="token punctuation">,</span><br>  LLMMetadata<span class="token punctuation">,</span><br><span class="token punctuation">)</span><br><span class="token keyword">from</span> llama_index<span class="token punctuation">.</span>core<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> llm_completion_callback<br><br>engine <span class="token operator">=</span> MLCEngine<span class="token punctuation">(</span>model<span class="token punctuation">)</span><br>    <br><span class="token keyword">class</span> <span class="token class-name">MLCLLM</span><span class="token punctuation">(</span>CustomLLM<span class="token punctuation">)</span><span class="token punctuation">:</span><br>  context_window<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> engine<span class="token punctuation">.</span>max_input_sequence_length<br>  num_output<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> engine<span class="token punctuation">.</span>engine_config<span class="token punctuation">.</span>max_num_sequence<br>  model_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> model<br><br>  <span class="token decorator annotation punctuation">@property</span><br>  <span class="token keyword">def</span> <span class="token function">metadata</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> LLMMetadata<span class="token punctuation">:</span><br>    <span class="token triple-quoted-string string">"""Get LLM metadata."""</span><br>    <span class="token keyword">return</span> LLMMetadata<span class="token punctuation">(</span><br>      context_window<span class="token operator">=</span>self<span class="token punctuation">.</span>context_window<span class="token punctuation">,</span><br>      num_output<span class="token operator">=</span>self<span class="token punctuation">.</span>num_output<span class="token punctuation">,</span><br>      model_name<span class="token operator">=</span>self<span class="token punctuation">.</span>model_name<span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><br><br>  <span class="token decorator annotation punctuation">@llm_completion_callback</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token keyword">def</span> <span class="token function">complete</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> CompletionResponse<span class="token punctuation">:</span><br>    response <span class="token operator">=</span> engine<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span><br>      messages<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span><br>      model<span class="token operator">=</span>model<span class="token punctuation">,</span><br>      stream<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><br>    text <span class="token operator">=</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">.</span>content<br>    <span class="token keyword">return</span> CompletionResponse<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">)</span><br><br>  <span class="token decorator annotation punctuation">@llm_completion_callback</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>  <span class="token keyword">def</span> <span class="token function">stream_complete</span><span class="token punctuation">(</span><br>      self<span class="token punctuation">,</span> prompt<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<br>  <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> CompletionResponseGen<span class="token punctuation">:</span><br>    full_response <span class="token operator">=</span> <span class="token string">""</span><br>    <span class="token keyword">for</span> response <span class="token keyword">in</span> engine<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span><br>      messages<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span><br>      model<span class="token operator">=</span>model<span class="token punctuation">,</span><br>      stream<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><br>    <span class="token punctuation">)</span><span class="token punctuation">:</span><br>      delta_response <span class="token operator">=</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>delta<span class="token punctuation">.</span>content<br>      full_response <span class="token operator">+=</span> delta_response<br>      <span class="token keyword">yield</span> CompletionResponse<span class="token punctuation">(</span>text<span class="token operator">=</span>full_response<span class="token punctuation">,</span> delta<span class="token operator">=</span>delta_response<span class="token punctuation">)</span></code></pre>
<p><strong>部署</strong> 如果必须使用大模型，就必须启动 Flask LLM 服务器进行启动；否则如果需要编译优化，启动普通 Flask 服务器即可；当然，也可以直接通过 index.html 或通过 live server 启动前端界面也可以进行操作。（当没有部署大模型时，会尝试使用前一节的 Serverless LLM 进行服务。）这主要受益于 PGFPlotsEdt 在架构上的分层设计，只将部分层赋予本地仍然可以进行服务。</p>
<pre class="mermaid">graph LR
    subgraph gunicorn-deploy.py
        direction LR
        A[Production Deployment]
        
        subgraph ppedt_server_llm.py
            direction LR
            B[LLM &#x26; Agent Pipeline]
            
            subgraph ppedt_server.py
                direction LR
                C[Server Interface]
            end
        end
    end

    A --&#x3E; B
    B --&#x3E; C
</pre>
<p>最终就可以直接通过 <a href="http://localhost:5678">http://localhost:5678</a> 本地访问，这也是最早实现的一种方式，本地部署可以尽可能地保证数据隐私。</p>
<h3>总结：降本增效</h3>
<p>PGFPlotsEdt 采用模式实现了对于这一场景 RAG 的部署实现，降低成本的同时，提升了生成效果。最近，有利用多模态大模型将示意图转换成 \(\rm\LaTeX\) TikZ（PGFPlots 的底层宏包）代码的工作 <a href="https://github.com/potamides/DeTikZify">DeTikZfy</a>；也有利用多模态大模型生成统计图的<a href="https://arxiv.org/pdf/2510.05091">工作</a>，其数据集包含了使用 \(\rm\LaTeX\) 绘制的图表。如何将多模态大模型集成进 PGFPlotsEdt 的场景中也是一个未来值得探讨的问题。</p>
<script type="module" async>import mermaid from "https://unpkg.com/mermaid@11.11.0/dist/mermaid.esm.min.mjs";document.addEventListener('DOMContentLoaded', mermaid.initialize({"loadOnSave":true}));</script>
</div>

    <footer>
        <div class="widthlimit">
            <p>版权所有 © Log Creative 2012-2024，保留所有权利。</p>
            <p>除有开源协议声明的，未经许可，不可用于商业用途。</p>
            <p>Powered by <a href="https://www.11ty.dev/" target="_blank">Eleventy</a>&nbsp;|&nbsp;<a href="https://beian.miit.gov.cn" target="_blank">沪ICP备2021032524号</a></p>
        </div>
    </footer>
</body>
</html>